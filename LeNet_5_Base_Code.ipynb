{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LeNet-5_Base_Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAM8-msW63Mn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "483d20cf-4022-48ed-9576-3405a458596b"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPool2D, AveragePooling2D\n",
        "from sklearn.metrics import classification_report\n",
        "import keras.optimizers\n",
        "from keras.utils import np_utils\n",
        "from sklearn import metrics \n",
        "from matplotlib import pyplot as plt\n",
        "from keras.regularizers import l2\n",
        "import tensorflow as tf\n",
        "from keras.models import model_from_json"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ur_uRJI7Tyr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "63a44222-661d-4fb0-ccdb-78e3a01e7554"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlvKpHmBQ7Mj",
        "colab_type": "text"
      },
      "source": [
        "Perpare data frame of Feature and Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU-MWLbv7UQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = \"/content/drive/My Drive/498L Project/data/\"\n",
        "\n",
        "SPECIES_LIST = [\"Parus major\", \"Phylloscopus collybita\", \"Baeolophus bicolor\", \"Melospiza melodia\", \"Cardinalis cardinalis\", \"Hirundo rustica\", \"Zenaida macroura\",\n",
        "                \"Spinus tristis\", \"Icterus galbula\", \"Sturnella magna\", \"Thryothorus ludovicianus\", \"Passer domesticus\", \"Poecile atricapillus\", \n",
        "                \"Turdus migratorius\", \"Cyanistes caeruleus\",\"Loxia curvirostra\", \"Passer montanus\", \"Sylvia atricapilla\", \"Sylvia communis\"]\n",
        "\n",
        "df = pd.DataFrame(columns=['Feature', 'Class_Label'])\n",
        "for j in range(0,len(SPECIES_LIST)):\n",
        "  features = []\n",
        "  species = SPECIES_LIST[j]\n",
        "  tensor_file_loc = DATA_DIR + species + \"/MFCC Tensor/feature_tensor.p\"\n",
        "  tensor = pickle.load( open( tensor_file_loc, \"rb\" ) )\n",
        "  length = len(tensor)\n",
        "  for i in range(0,length):\n",
        "    row = [tensor[i], species]\n",
        "    df.loc[len(df)] = row"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh8u6FBvRMv5",
        "colab_type": "text"
      },
      "source": [
        "Doing one hot encoding and splitting to train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmcBhV0XRSS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.array(df.Feature.tolist())\n",
        "y = np.array(df.Class_Label.tolist())\n",
        "le = LabelEncoder()\n",
        "yy = to_categorical(le.fit_transform(y)) \n",
        "x_training, x_test, y_training, y_test = train_test_split(X, yy, test_size=0.15, random_state = 42)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_training, y_training, test_size=0.1, random_state = 42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt-arnk0SAxa",
        "colab_type": "text"
      },
      "source": [
        "Building the Model (LeNet-5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W042BG4cSI55",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "1fdbd558-006a-4630-d910-4874de5faeb7"
      },
      "source": [
        "num_rows = 20\n",
        "num_columns = 1500\n",
        "num_channels = 1\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
        "x_val = x_val.reshape(x_val.shape[0], num_rows, num_columns, num_channels)\n",
        "x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
        "\n",
        "num_labels = yy.shape[1]\n",
        "stride = 2\n",
        "kernal = 2\n",
        "# Construct model \n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=64, kernel_size=kernal, input_shape=(num_rows, num_columns, num_channels),activation='tanh'))#,kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n",
        "model.add(AveragePooling2D(pool_size=2, strides=(stride,stride)))\n",
        "#model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=kernal, activation='tanh'))#,kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n",
        "model.add(AveragePooling2D(pool_size=2, strides=(stride,stride)))\n",
        "#model.add(Dropout(0.2))\n",
        "\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(150, activation='tanh'))#,kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(100, activation='tanh'))#,kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(num_labels, activation='softmax'))#,kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 19, 1499, 64)      320       \n",
            "_________________________________________________________________\n",
            "average_pooling2d_3 (Average (None, 9, 749, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 8, 748, 128)       32896     \n",
            "_________________________________________________________________\n",
            "average_pooling2d_4 (Average (None, 4, 374, 128)       0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 150)               19350     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               15100     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 19)                1919      \n",
            "=================================================================\n",
            "Total params: 69,585\n",
            "Trainable params: 69,585\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eaj9VTVAS3VY",
        "colab_type": "text"
      },
      "source": [
        "Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS6tyI2cUOpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model\n",
        "opt = keras.optimizers.Adam(learning_rate=0.005)\n",
        "#opt = keras.optimizers.SGD(lr=0.015,momentum=0.5)\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
        "\n",
        "num_epochs = 75\n",
        "num_batch_size = 100\n",
        "history = model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_val, y_val), verbose=1,shuffle=True)\n",
        "\n",
        "# Calculate accuracy \n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "accuracy = 100*score[1]\n",
        "\n",
        "Y_test = np.argmax(y_test, axis=1) # Convert one-hot to index\n",
        "y_pred = model.predict_classes(x_test)\n",
        "print(classification_report(Y_test, y_pred))\n",
        "\n",
        "print(\"Overall Accuracy: %.4f%%\" % accuracy) \n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f63pobveWZMy",
        "colab_type": "text"
      },
      "source": [
        "Saving the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQHvd-yJWZjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_json = model.to_json()\n",
        "with open(\"Base_LeNet_Model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"Base_LeNet_Model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}